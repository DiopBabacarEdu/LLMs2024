
# ğŸ“„ Attention Is All You Need

**Auteurs** : *Vaswani et al.*  
**Date de publication** : *2017*  
**Contexte** : Cet article a introduit le modÃ¨le **Transformer**, une architecture rÃ©volutionnaire pour les tÃ¢ches de traitement du langage naturel (**NLP**).

---

## ğŸš€ ProblÃ¨me abordÃ©

- Avant le **Transformer**, les modÃ¨les pour les tÃ¢ches sÃ©quence-Ã -sÃ©quence (comme la traduction) utilisaient principalement des **RÃ©seaux de Neurones RÃ©currents (RNN)** et des **LSTMs**.
- **Limites** de ces approches :
  - **CoÃ»teuses** en calcul.
  - DifficultÃ©s de **parallÃ©lisation**.
  - Mauvaise gestion des **dÃ©pendances Ã  long terme** dans les sÃ©quences.

---

## ğŸ’¡ Contribution principale

Les auteurs proposent une **nouvelle architecture** : le **Transformer**.

- **CaractÃ©ristique clÃ©** : Il n'utilise **ni rÃ©currence** ni **convolution**.
- Il repose entiÃ¨rement sur le mÃ©canisme **self-attention** (attention par produit scalaire pondÃ©rÃ©).

---

## ğŸ”‘ Innovation : Self-Attention

Le mÃ©canisme **self-attention** permet de pondÃ©rer les relations entre les mots d'une sÃ©quence indÃ©pendamment de leur distance.

### ğŸ§© Fonctionnement :

1. **Calcul des reprÃ©sentations clÃ©s** :
   Chaque mot est reprÃ©sentÃ© par 3 vecteurs :  
   - **Query** (Q)  
   - **Key** (K)  
   - **Value** (V)  

2. **Produit scalaire** :
   La similaritÃ© entre les mots est mesurÃ©e avec un produit scalaire entre **Q** et **K**, puis normalisÃ©e avec **Softmax**.

3. **PondÃ©ration des valeurs** :
   Le rÃ©sultat est utilisÃ© pour pondÃ©rer les **V** (valeurs).

---

## âš™ï¸ Architecture du Transformer

L'architecture est constituÃ©e de **6 couches** dans l'encodeur et le dÃ©codeur, avec des mÃ©canismes d'attention multi-tÃªtes.

### ğŸ“Œ Encodeur

- **Self-Attention** : Chaque mot dans une sÃ©quence peut s'attarder sur les autres mots pour capturer leurs relations.
- **Feed-Forward Network** : Des couches denses pour le traitement des reprÃ©sentations.

### ğŸ“Œ DÃ©codeur

- **Masked Self-Attention** : Similaire Ã  l'encodeur mais masque les mots futurs pour Ã©viter les fuites d'informations.
- **Cross-Attention** : Relie les reprÃ©sentations du dÃ©codeur aux sorties de l'encodeur.

---

## ğŸ“Š Avantages du Transformer

1. **ParallÃ©lisation** :  
   Contrairement aux **RNN**, toutes les positions peuvent Ãªtre calculÃ©es simultanÃ©ment. Cela rÃ©duit considÃ©rablement le temps d'entraÃ®nement.

2. **Gestion des longues dÃ©pendances** :  
   Le mÃ©canisme **self-attention** permet de relier directement tous les mots d'une sÃ©quence.

3. **FlexibilitÃ©** :  
   Le modÃ¨le s'applique Ã  de nombreuses tÃ¢ches NLP : traduction, rÃ©sumÃ© automatique, gÃ©nÃ©ration de texte, etc.

---

## ğŸŒ Applications et impact

Le Transformer a rÃ©volutionnÃ© le NLP et est Ã  la base de modÃ¨les populaires tels que :  
- **BERT** : Pour l'analyse de texte et l'extraction d'informations.  
- **GPT** : Pour la gÃ©nÃ©ration de texte.  
- **T5** : Pour les tÃ¢ches de traduction et de rÃ©sumÃ©.  

---

## ğŸ“ Conclusion

L'architecture **Transformer** introduite par *"Attention Is All You Need"* a marquÃ© un tournant dans le **traitement du langage naturel** grÃ¢ce Ã  son efficacitÃ©, sa parallÃ©lisation et son utilisation ingÃ©nieuse du mÃ©canisme **self-attention**.

